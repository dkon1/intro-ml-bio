---
title: "Linear regression (lab 3 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to use linear regression for both single and multiple variables. You will do the following:

1.  Calculate single variable linear regression and assess its quality

2.  Compare errors for training and test set

3.  Use polynomial regression and assess its quality

4.  Check for overfitting and select appropriate model

```{r setup}
#| include: false
#| echo: false
library(tidyverse)

```

## Heart rates

The following data set contains heart rates measured and reported by students in my class Introduction to Quantitative Modeling for Biology. There are four different heart rates measured (two at rest and two after exercise) and the year it was measured.

```{r}
heart_rates <- read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/HR_data_combined.csv")
```

1.  Select a response and an explanatory variable (from the 4 heart rate variables) and clean the data to remove any outliers or missing values in these variables. Split the data set into training and test sets of equal size.

    ```{r}
    heart_data <- heart_rates |> 
      dplyr::select(Rest2, Ex2) |> 
      drop_na() 


    train_index <- sample(nrow(heart_data), size = floor(0.5 * nrow(heart_data)))

    heart_train <- heart_data |>  
      slice(train_index) |> arrange(Rest2)

    heart_test <- heart_data |>  
      slice(-train_index) |> arrange(Rest2)

    ```

2.  Use linear regression on the training set and print out the summary. Make a scatterplot and overlay the linear regression model as a line on the same plot. Make a separate plot of the residuals on the training set, and assess whether it looks good (like a shapeless blob). Based on the summary, explain which parameters are significantly different from zero, according to the hypothesis test.

```{r}

lm_out <- lm(Ex2 ~ Rest2, data = heart_train)

summary(lm_out)
# base R:
plot(Ex2 ~ Rest2, data = heart_train, cex = .8, col = "blue", main = paste("Linear regression over heart data"))
abline(lm_out)

#produce residual vs. fitted plot
plot(fitted(lm_out), resid(lm_out))

#add a horizontal line at 0 
abline(0,0)

# ggplot:
 heart_train |> ggplot() + 
  aes(x = Rest2, y = Ex2) + geom_point(color = 'blue') +
  geom_smooth(method = 'lm', color = 'darkorange') + ggtitle(paste("Linear regression over heart data"))
 
 ggplot(lm_out, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  ggtitle(paste("Residuals of regression on heart data"))

```

The outliers look like a shapeless blob, and the p-values for both the slope and intercept are extremely small, indicating they're certainly different from zero.

3.  Perform quadratic regression on the training set using `lm` and print out the summary. Make a separate plot of the residuals on the training set, and assess whether it looks good (like a shapeless blob). Based on the summary, explain which parameters are significantly different from zero, according to the hypothesis test, and report whether the fit is improved compared to the linear model.

    ```{r}

    quad_out <- lm(Ex2 ~ poly(Rest2, degree = 2, raw = TRUE), data = heart_train)
    summary(quad_out)

    # base R:
    plot(Ex2 ~ Rest2, data = heart_train, cex = .8, col = "blue", main = paste("Quadratic regression on heart rate data"))

    y_pred <- quad_out$coefficients[1] + quad_out$coefficients[2]*heart_train$Rest2 + quad_out$coefficients[3]*heart_train$Rest2^2

    lines(heart_train$Rest2, y_pred, lwd = 3, col = 'darkorange')

    #produce residual vs. fitted plot
    plot(fitted(quad_out), resid(quad_out), main = paste("Residuals of quadratic regression on heart data"))
    #add a horizontal line at 0 
    abline(0,0)

    # ggplot:
    heart_train |> ggplot() + 
      aes(x = Rest2, y = Ex2) + geom_point(color = 'blue') +
      geom_smooth( method = 'lm', formula = 'y ~ poly(x, 2, raw=TRUE)', color = 'darkorange') + ggtitle(paste("Quadratic regression"))
     
    ggplot(quad_out, aes(x = .fitted, y = .resid)) +
      geom_point() +
      geom_hline(yintercept = 0) +
      ggtitle(paste("Residuals of quadratic regression on heart data"))

     
    ```

    The outliers still look good, and the r-squared is improved a bit (by 1-2% in most cases, depending on the random split). The parameters are usually significant, though the quadratic coefficient is very small.

4.  Use the linear regression model parameters to compute the predicted values for the test set, and calculate the residuals for the test set. Print the variance of the residuals for the training and test sets, and compare them. Use the quadratic regression model to compute the predicted values for the test set using the quadratic regression coefficients, and calculate the residuals for the test set. Print the variance of the residuals for the training and test sets, and compare them. How did adding the quadratic parameter impact the error for the training and test sets?

    ```{r}
    print(paste("Residual variance from linear model for the training set:", var(lm_out$residuals)))
     
    y_pred <- lm_out$coefficients[1] + lm_out$coefficients[2]*heart_test$Rest2

    print(paste("Residual variance from linear model for the test set:", var(y_pred - heart_test$Ex2))) 

    print(paste("Residual variance from quadratic model for the training set:", var(quad_out$residuals)))
     
    y_pred <- quad_out$coefficients[1] + quad_out$coefficients[2]*heart_test$Rest2 + quad_out$coefficients[3]*heart_test$Rest2^2

    print(paste("Residual variance from quadratic model for the for the test set:", var(y_pred - heart_test$Ex2))) 
    ```

    Neither the training nor the test set residual variance is improved dramatically by adding the quadratic terms, but for most splits, the training set is improved more, and sometimes the error in the test set goes up, indicating potential overfitting (though nothing dramatic).

5.  Redo the random split of the original data set into training and test sets, leaving progressively smaller fractions of the data in the training set, and calculate linear regression on the training set. Calculate and print the variance of the residuals of the training and test sets, and report whether you observe overfitting.

```{r}

train_index <- sample(nrow(heart_data), size = floor(0.01 * nrow(heart_data)))

heart_train <- heart_data |>  
  slice(train_index) 

heart_test <- heart_data |>  
  slice(-train_index) 

lm_out <- lm(Ex2 ~ Rest2, data = heart_train)

summary(lm_out)

print(paste("Residual variance for the 1% training set", var(lm_out$residuals)))
 
y_pred <- lm_out$coefficients[1] + lm_out$coefficients[2]*heart_test$Rest2

print(paste("Residual variance for the 99% test set", var(y_pred - heart_test$Ex2))) 
```

It really depends on the luck of the draw! For some random selections of training data at 5% or 10%, we observe overfitting (the error in the test set is significantly higher than in the training set), but not always. With 1% split, there's almost always overfitting, though sometimes it goes the other way. I believe this has to do with the outliers I didn't filter out.

### Ecological data

The following data set contains observations of the populations of one species of fish (cutthroat trout) and two species of salamander in Mack Creek, Andrews Forest, Willamette National Forest, Oregon. The data set contains 16 variables and over thirty-two thousand observations. The variables include time and date, location, and measurements, such as size and weight. The metadata (descriptions of data) are provided [here](https://portal.edirepository.org/nis/metadataviewer?packageid=knb-lter-and.4027.14) (click on "Data entities" tab for explanations of each variable.)

```{r}
mack_data <- read_csv("https://raw.githubusercontent.com/dkon1/quant_life_quarto/main/data/mack_data.csv")
```

1.  You will use the numeric variables `length_1_mm` and `weight_g`, examine the data visually to see if there are any missing values or outliers. Clean the data to remove any values you want, and filter the data to contain observations from only one species, either: 'Cutthroat trout' or 'Coastal giant salamander'. Split the remaining data into training and test sets of equal size.

    ```{r}
    mack_data <- mack_data |> dplyr::select(species, length_1_mm, weight_g) |> 
      drop_na() |>  filter (species == 'Cutthroat trout')

    train_index <- sample(nrow(mack_data ), size = floor(0.5 * nrow(mack_data )))

    mack_train <- mack_data  |>  
      slice(train_index) |> 
      arrange(length_1_mm)

    mack_test <- mack_data  |>  
      slice(-train_index) |> 
       arrange(length_1_mm)


    ```

2.  Use linear regression on the training set and print out the summary. Make a scatterplot and overlay the linear regression model as a line on the same plot. Make a separate plot of the residuals on the training set, and assess whether it looks good (like a shapeless blob). Based on the summary, explain which parameters are significantly different from zero, according to the hypothesis test.

```{r}
lm_out <- lm(weight_g ~ length_1_mm , data = mack_train)

summary(lm_out)


# base R:
plot(weight_g ~ length_1_mm, data = mack_train, cex = .8, col = "blue", main = paste("Linear regression on mack data"))
abline(lm_out)


#produce residual vs. fitted plot
plot(fitted(lm_out), resid(lm_out), main = "Residuals of linear regression on mack data")

#add a horizontal line at 0 
abline(0,0)


# ggplot:
 mack_train |> ggplot() + 
  aes(x = length_1_mm, y = weight_g) + geom_point(color = 'blue') +
  geom_smooth( method = 'lm', color = 'darkorange') + ggtitle(paste("Linear regression on mack data"))
 
 ggplot(lm_out, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  ggtitle(paste("Residuals of linear regression on mack data"))


```

No, the residuals do not look like a shapeless blob! This indicated an essential nonlinearity in the data, and thus linear model is not appropriate, despite a high r-squared of over 80%. All parameters are "significant" as indicated by their low p-values.

3.  Perform quadratic regression on the training set using `lm` and print out the summary. Make a separate plot of the residuals on the training set, and assess whether it looks good (like a shapeless blob). Based on the summary, explain which parameters are significantly different from zero, according to the hypothesis test, and report whether the fit is improved compared to the linear model.

```{r}
quad_out <- lm(weight_g ~ poly(length_1_mm, degree = 2, raw = TRUE), data = mack_train)
summary(quad_out)


# base R:
plot(weight_g ~ length_1_mm, data = mack_train, cex = .8, col = "blue", main = paste("Quadratic regression on mack data"))

y_pred <- quad_out$coefficients[1] + quad_out$coefficients[2]*mack_train$length_1_mm + quad_out$coefficients[3]*mack_train$length_1_mm^2

lines(mack_train$length_1_mm, y_pred, lwd = 3, col = 'darkorange')


#produce residual vs. fitted plot
plot(fitted(quad_out), resid(quad_out), main = "Residuals of quadratic regression on mack data")

#add a horizontal line at 0 
abline(0,0)


# ggplot:
 mack_train |> ggplot() + 
  aes(x = length_1_mm, y = weight_g) + geom_point(color = 'blue') +
  geom_smooth( method = 'lm', formula = 'y ~ poly(x, 2, raw=TRUE)', color = 'darkorange') + ggtitle(paste("Quadratic regression on mack data"))

 
  ggplot(quad_out, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0) +
  ggtitle(paste("Residuals of linear regression on mack data"))

```

The residuals now look shapeless, and the r-squared improves considerably (by 10% or more). All the parameters are still significant.

4.  Use the linear regression model parameters to compute the predicted values for the test set, and calculate the residuals for the test set. Print the variance of the residuals for the training and test sets, and compare them. Use the quadratic regression model to compute the predicted values for the test set using the quadratic regression coefficients, and calculate the residuals for the test set. Print the variance of the residuals for the training and test sets, and compare them. How did adding the quadratic parameter impact the error for the training and test sets?

```{r}
print(paste("Residual variance for the linear model on the training set", var(lm_out$residuals)))
 
y_pred <- lm_out$coefficients[1] + lm_out$coefficients[2]*mack_test$length_1_mm

print(paste("Residual variance for the linear model on the  test set", var(y_pred - mack_test$weight_g))) 

print(paste("Residual variance for the quadratic model on the training set", var(quad_out$residuals)))


y_pred <- quad_out$coefficients[1] + quad_out$coefficients[2]*mack_test$length_1_mm + quad_out$coefficients[3]*mack_test$length_1_mm^2

print(paste("Residual variance for quadratic model on the test set", var(y_pred - mack_test$weight_g))) 
```

The error improved dramatically for both the training and the test sets with addition of the quadratic parameter!

5.  Perform a cubic polynomial fit on the same data, print out the summary, and produce the same plots as before. Then compute and print the variance of the residuals for the training and test sets, and compare them. How did adding the cubic parameter impact the error for the training and test sets?

    ```{r}
    cub_out <- lm(weight_g ~ poly(length_1_mm, degree = 3), data = mack_train)
    summary(cub_out)

    # base R:
    plot(weight_g ~ length_1_mm, data = mack_train, cex = .8, col = "blue", main = paste("Cubic regression on mack data"))

    y_pred <- cub_out$coefficients[1] + cub_out$coefficients[2]*mack_train$length_1_mm + cub_out$coefficients[3]*mack_train$length_1_mm^2 + cub_out$coefficients[4]*mack_train$length_1_mm^3

    lines(mack_train$length_1_mm,fitted(cub_out), lwd = 3, col = 'darkorange')


    #produce residual vs. fitted plot
    plot(fitted(cub_out), resid(cub_out), main = "Residuals of cubic regression on mack data")

    #add a horizontal line at 0 
    abline(0,0)


    # ggplot:
     mack_train |> ggplot() + 
      aes(x = length_1_mm, y = weight_g) + geom_point(color = 'blue') +
      geom_smooth( method = 'lm', formula = 'y ~ poly(x, degree = 3, raw = TRUE)', color = 'darkorange') + ggtitle(paste("Cubic regression on mack data"))

     
     cub_out |>  ggplot() + aes(x = .fitted, y = .resid) +
      geom_point() +
      geom_hline(yintercept = 0) +
      ggtitle(paste("Residuals of cubic regression on mack data"))


    print(paste("Residual variance for the cubic model on the training set", var(cub_out$residuals)))


    y_pred <- cub_out$coefficients[1] + cub_out$coefficients[2]*mack_test$length_1_mm + cub_out$coefficients[3]*mack_test$length_1_mm^2 + cub_out$coefficients[4]*mack_test$length_1_mm^3

    print(paste("Residual variance for cubic model on the test set", var(y_pred - mack_test$weight_g))) 
     
    ```

Adding a cubic term improves the error for the training set by a small amount, but it completely blows up the error for the test set! This is a dramatic example of overfitting.
