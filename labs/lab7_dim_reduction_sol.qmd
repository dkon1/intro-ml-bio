---
title: "Unsupervised learning: PCA and clustering (lab 7 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to use methods of unsupervised learning, specifically:

-   PCA for dimensionality reduction

-   clustering using k-means and hierarchical methods

-   clustering using reduced dimensions

-   validate clustering and compare different methods

```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(ggmap) # for ggimage
library(ggfortify) # for autoplot
library(factoextra)
library(NbClust)
library(tidyclust)
library(janitor)
```

## Part 1: Dimensionality reduction using PCA

## Breast cancer data

The Wisconsin breast cancer data set contains information about different tumor biopsies in the form of 30 features of cell nuclei determined from images. The first two variables are the sample ID and the diagnosis (M = malignant or B = benign). The source and description of the data is here: https://archive-beta.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+diagnostic

```{r}
wisc_data = read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/wdbc.csv")

```

1.  Clean the data set by filtering out any missing values, selecting out the non-predictor variables and fixing a few variable names that have weird characters (the easiest way to do this is to use the function `clean_names()` from the package `janitor`) and assign this to a new tibble. Assign the `Diagnosis` variable into a separate data frame.

    ```{r}
    wisc_pred<- wisc_data |> 
     dplyr::select(-c(ID, Diagnosis)) |> 
     clean_names() |> 
     drop_na()

    wisc_diag <- wisc_data |> 
     dplyr::select(Diagnosis) |> 
     drop_na()
    ```

2.  Perform PCA on the raw (unscaled) data and make a Scree plot of the eigenvalues. Plot the data points projected onto the first two principalcomponents colored by their Diagnosis.

```{r}
wisc_pca <- wisc_pred  |> 
  prcomp(scale = FALSE)

tidy(wisc_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
  ggplot(aes(PC, percent)) +
  geom_col()

fviz_pca_ind(wisc_pca,
             col.ind = factor(wisc_diag$Diagnosis),
             label = 'none',
             addEllipses=TRUE, 
             ellipse.level=0.9, 
             palette = "Dark2")
```

3.  Perform PCA on the scaled variables and make a Scree plot of the eigenvalues. Plot the data points projected onto the first two principalcomponents colored by their Diagnosis.

    ```{r}
    wisc_pca <- wisc_pred  |> 
      prcomp(scale = TRUE)

    tidy(wisc_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
      ggplot(aes(PC, percent)) +
      geom_col()

    fviz_pca_ind(wisc_pca,
                 col.ind = factor(wisc_diag$Diagnosis),
                 label = 'none',
                 addEllipses=TRUE, 
                 ellipse.level=0.9, 
                 palette = "Dark2")
    ```

4.  Discuss the PCA results on the scaled and unscaled Wisconsin data by comparing the fraction of variance captured by the first two PCs, and by comparing the plot of the data points projected onto the first two PCs.

YOUR ANSWER HERE

### Liver injury data

The following data set contains multiple (376) numeric variables based on assays of different chemical compounds in patients (predictors) and a categorical variable `class` describing the degree of liver damage: 'none', 'mild', 'severe'.

```{r}
data("hepatic_injury_qsar", package = "modeldata")
#glimpse(hepatic_injury_qsar)
```

5.  Clean the data to remove any missing values, and select only the **predictor variables that are non-constant (with nonzero variance)** to be used for clustering and assign them to a new tibble. Assign the variable `class` to a separate tibble.

    ```{r}
    # identify variables with zero variance
    zerovar <- hepatic_injury_qsar |>  dplyr::select(-class) |> 
      summarise_all(var) |>  
      select_if(function(.) . == 0) |> 
      names()

    liver_clean <- hepatic_injury_qsar |> dplyr::select(-class, -all_of(zerovar)) |> 
      drop_na()

    liver_class <- hepatic_injury_qsar |>  dplyr::select(class) |> 
      drop_na() |>
      mutate(class = factor(class))

    ```

6.  Perform PCA on the raw (unscaled) data and make a Scree plot of the eigenvalues. Plot the data points projected onto the first two principal components colored by `class`.

    ```{r}
    liver_pca <- liver_clean  |> 
      prcomp(scale = FALSE)

    tidy(liver_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
      ggplot(aes(PC, percent)) +
      geom_col()

    fviz_pca_ind(liver_pca,
                 col.ind = factor(liver_class$class),
                 label = 'none',
                 addEllipses=TRUE, 
                 ellipse.level=0.9, 
                 palette = "Dark2")
    ```

7.  Perform PCA on the scaled variables and make a Scree plot of the eigenvalues. Plot the data points projected onto the first two principalcom ponents colored by their `class`.

    ```{r}
    liver_pca <- liver_clean  |> 
      prcomp(scale = TRUE)

    tidy(liver_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
      ggplot(aes(PC, percent)) +
      geom_col()

    fviz_pca_ind(liver_pca,
                 col.ind = factor(liver_class$class),
                 label = 'none',
                 addEllipses=TRUE, 
                 ellipse.level=0.9, 
                 palette = "Dark2")
    ```

8.  Discuss the PCA results on the scaled and unscaled liver injury data by comparing the fraction of variance captured by the first two PCs, and by comparing the plot of the data points projected onto the first two PCs.

## Part 2: clustering

### Breast cancer data

1.  Perform k-means clustering with k=2 on the Wisconsin data set using all the raw variables. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the `Diagnosis` value for all the points, and report the number of points assigned to the "wrong" cluster (since cluster labels are arbitrary, either cluster 1 or 2 may correspond to Diagnosis 'M' or 'B').

```{r}
kmeans_spec <- k_means(num_clusters = 2) |>
  set_mode("partition") |>
  set_engine("stats") |>
  set_args(nstart = 20)


kmeans_fit <- kmeans_spec |> 
  fit(~., data = wisc_pred)

wisc_km <- augment(kmeans_fit, new_data = wisc_pred) 

fviz_cluster(list(data = wisc_pred, cluster = wisc_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(wisc_diag$Diagnosis, wisc_km$.pred_cluster)
```

2.  Perform k-means clustering with k=2 on this data set using only the first few PCs from PCA (you can do this by creating a recipe with `step_pca`) on the **unscaled** data. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the Diagnosis value for all the points and report the number of points assigned to the "wrong" cluster (since cluster labels are arbitrary, either cluster 1 or 2 may correspond to Diagnosis 'M' or 'B').

```{r}
kmeans_rec <- recipe(~., data = wisc_pred) |> 
  #step_normalize(all_numeric()) |> 
  step_pca(all_numeric(), num_comp = 2) #|>
#  prep() |>
 # bake(new_data = NULL)

workflow_km <- workflow() |> 
  add_model(kmeans_spec) |>
  add_recipe(kmeans_rec)

kmeans_fit <-  workflow_km |> 
  fit(wisc_pred)

wisc_km <- augment(kmeans_fit, new_data = wisc_pred) 

fviz_cluster(list(data = wisc_pred, cluster = wisc_diag$Diagnosis),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(wisc_diag$Diagnosis, wisc_km$.pred_cluster)
```

4.  Perform k-means clustering with k=2 on this data set using only the first few PCs from PCA (you can do this by creating a recipe with `step_pca`) on the **scaled** data. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the Diagnosis value for all the points and report the number of points assigned to the "wrong" cluster (since cluster labels are arbitrary, either cluster 1 or 2 may correspond to Diagnosis 'M' or 'B').

    ```{r}
    kmeans_rec <- recipe(~., data = wisc_pred) |> 
      step_normalize(all_numeric()) |> 
      step_pca(all_numeric(), threshold = 0.7) #|>
    #  prep() |>
     # bake(new_data = NULL)

    workflow_km <- workflow() |> 
      add_model(kmeans_spec) |>
      add_recipe(kmeans_rec)

    kmeans_fit <-  workflow_km |> 
      fit(wisc_pred)

    wisc_km <- augment(kmeans_fit, new_data = wisc_pred) 

    fviz_cluster(list(data = scale(wisc_pred), cluster = wisc_diag$Diagnosis),
    ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

    table(wisc_diag$Diagnosis, wisc_km$.pred_cluster)
    ```

5.  Choose either the complete set of variables or the reduced set of principal components, and perform hyperparameter tuning to find the optimal value of k in k-means using `tidymodels` tools. Make a plot of within SSE (sum of squared errors within clusters) as a function of k (try values between 1 and 10) and use the elbow method to determine the optimal value of k.

```{r}
kmeans_spec_tuned <- kmeans_spec |> 
  set_args(num_clusters = tune())


pca_recipe <- 
  recipe(formula =  ~ ., data = wisc_pred) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) |>
  step_pca(all_predictors(), threshold = 0.8) 

kmeans_wf <- workflow() |>
  add_model(kmeans_spec_tuned) |>
  add_recipe(pca_recipe)

splits <- bootstraps(wisc_pred, times = 10)

num_clusters_grid <- tibble(num_clusters = seq(1, 10))

tune_res <- tune_cluster(
  object = kmeans_wf,
  resamples = splits,
  grid = num_clusters_grid
)

tune_res |>
  collect_metrics()

tune_res |>
  autoplot()
```

6.  Compare the performance of clustering with and without reducing dimensionality, in terms of its agreement with the true labels (from Diagnosis). Explain the meaning of the elbow method for hyperparameter tuning and whether the result is surprising.

K-means does a better job with reduced dimensions (2 PCs) and scaled variables than with the full data set; the total number of mislabeled points goes down (this will vary a bit since k-means is random, but I got 81 mislabeled points from the raw data and 53 from the scaled and reduced data). From the elbow plot, it appears that 2 is the optimal number of clusters; the elbow method essentially assumes that at the optimal number of clusters the within cluster variance decreases the fastest and for larger k we see diminishing returns.

## Liver injury data

7.  Perform k-means clustering with k=3 on the liver injury data set using all the scaled variables. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the `class` value for all the points.

```{r}
kmeans_spec <- k_means(num_clusters = 3) |>
  set_mode("partition") |>
  set_engine("stats") |>
  set_args(nstart = 20)


liver_rec <- liver_clean |> 
  recipe(formula =  ~ .) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) 

kmeans_wf <- workflow() |>
  add_model(kmeans_spec) |>
  add_recipe(liver_rec)

kmeans_fit <- kmeans_wf  |> 
  fit(liver_clean)

liver_km <- predict(kmeans_fit, new_data = liver_clean)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(liver_km$.pred_cluster, liver_class$class)
```

8.  Perform k-means clustering with k=3 on the liver injury data set using the scaled and PCA-reduced dimensions (use the function `step_pca()` with the option `threshold` to specify the fraction of variance you want retain in the reduced dimensions. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the `class` value for all the points.

```{r}
liver_rec <- liver_clean |> 
  recipe(formula =  ~ .) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) |> 
  step_pca(all_predictors(), threshold = 0.7) 

kmeans_wf <- workflow() |>
  add_model(kmeans_spec) |>
  add_recipe(liver_rec)

kmeans_fit <- kmeans_wf  |> 
  fit(liver_clean)

liver_km <- predict(kmeans_fit, new_data = liver_clean)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(liver_km$.pred_cluster, liver_class$class)
```

9.  Use the reduced set of principal components and perform hyperparameter tuning to find the optimal value of k in k-means using `tidymodels` tools. Make a plot of within SSE (sum of squared errors within clusters) as a function of k (try values between 1 and 10) and use the elbow method to determine the optimal value of k.

    ```{r}
    kmeans_spec_tuned <- kmeans_spec |> 
      set_args(num_clusters = tune())


    pca_recipe <- 
      recipe(formula =  ~ ., data = liver_clean) |> 
      step_novel(all_nominal_predictors()) |> 
      step_dummy(all_nominal_predictors()) |> 
      step_zv(all_predictors()) |> 
      step_normalize(all_predictors()) |>
      step_pca(all_predictors(), threshold = 0.7) 

    kmeans_wf <- workflow() |>
      add_model(kmeans_spec_tuned) |>
      add_recipe(pca_recipe)

    splits <- bootstraps(liver_clean, times = 10)

    num_clusters_grid <- tibble(num_clusters = seq(1, 10))

    tune_res <- tune_cluster(
      object = kmeans_wf,
      resamples = splits,
      grid = num_clusters_grid
    )

    tune_res |>
      collect_metrics()

    tune_res |>
      autoplot()
    ```

10. Perform hierarchical clustering on the same data set using the method "single" using the scaled and PCA-reduced dimensions (use the function `step_pca()` with the option `threshold` to specify the fraction of variance you want retain in the reduced dimensions. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the `class` value for all the points.

```{r}
hclust_spec <- hier_clust(linkage_method = "single")
  
hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(liver_rec)

hclust_fit <- hclust_wf  |> 
  fit(liver_clean)

liver_hc <- predict(hclust_fit, new_data = liver_clean, num_clusters = 3)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using single linkage method:")
table(liver_hc$.pred_cluster, liver_class$class)
```

11. Try using other linkage methods (e.g. "complete" and "average") and report the same confusion matrix for both.

```{r}
hclust_spec <- hier_clust(linkage_method = "complete")
  
hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(liver_rec)

hclust_fit <- hclust_wf  |> 
  fit(liver_clean)

liver_hc <- predict(hclust_fit, new_data = liver_clean, num_clusters = 3)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())


print("The confusion matrix for hierarchical clustering using complete linkage method:")
table(liver_hc$.pred_cluster, liver_class$class)
```

```{r}
hclust_spec <- hier_clust(linkage_method = "average")
  
hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(liver_rec)

hclust_fit <- hclust_wf  |> 
  fit(liver_clean)

liver_hc <- predict(hclust_fit, new_data = liver_clean, num_clusters = 3)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using average linkage method:")
table(liver_hc$.pred_cluster, liver_class$class)
```

12. Report how the clustering quality (as judged by comparison with the true value of `class`) depends on the type of clustering algorithm.

    This is a very tough data set to cluster, and hierarchical methods fail completely; the k-means does only about as good as a coin toss.
