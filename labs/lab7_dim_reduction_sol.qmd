---
title: "Unsupervised learning: PCA and clustering (lab 7 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to use methods of unsupervised learning, specifically:

-   PCA for dimensionality reduction

-   clustering using k-means and hierarchical methods

-   clustering using reduced dimensions

-   validate clustering and compare different methods

```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(ggmap) # for ggimage
library(ggfortify) # for autoplot
library(factoextra)
library(NbClust)
library(tidyclust)
library(janitor)
```

## Breast cancer data

The Wisconsin breast cancer data set contains information about different tumor biopsies in the form of 30 features of cell nuclei determined from images. The first two variables are the sample ID and the diagnosis (M = malignant or B = benign). The source and description of the data is here: https://archive-beta.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+diagnostic

```{r}
wisc_data = read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/wdbc.csv")

```

1.  Clean the data set by removing the non-predictor variables (hint: to make things run smoothly you should also fix a few weird variable names!) and removing any missing values, and place the Diagnosis variable into a separate data frame. Perform PCA on the data (after scaling the variables), and make a Scree plot of the eigenvalues. Plot the data points projected onto the first principal components colored by their Diagnosis.

```{r}
wisc_pred<- wisc_data |> 
 dplyr::select(-c(ID, Diagnosis)) |> 
 clean_names() |> 
 drop_na()

wisc_class <- wisc_data |> 
 dplyr::select(Diagnosis) |> 
 drop_na()

wisc_pca <- wisc_pred  |> 
  prcomp(scale = TRUE)


tidy(wisc_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
  ggplot(aes(PC, percent)) +
  geom_col()

fviz_pca_ind(wisc_pca,
             col.ind = factor(wisc_class$Diagnosis),
             label = 'none',
             addEllipses=TRUE, 
             ellipse.level=0.9, 
             palette = "Dark2")
```

2.  Perform k-means clustering with k=2 on this data set using all the scaled variables. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the Diagnosis value for all the points.

```{r}
kmeans_spec <- k_means(num_clusters = 2) |>
  set_mode("partition") |>
  set_engine("stats") |>
  set_args(nstart = 20)


kmeans_fit <- kmeans_spec |> 
  fit(~., data = wisc_pred)

wisc_km <- augment(kmeans_fit, new_data = wisc_pred) 

fviz_cluster(list(data = scale(wisc_pred), cluster = wisc_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(wisc_class$Diagnosis, wisc_km$.pred_cluster)
```

3.  Perform k-means clustering with k=2 on this data set using only the first few PCs from PCA (you can do this by creating a recipe with `step_pca`). Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the Diagnosis value for all the points.

```{r}

kmeans_spec <- k_means(num_clusters = 2) |>
  set_mode("partition") |>
  set_engine("stats") |>
  set_args(nstart = 20)


kmeans_rec <- recipe(~., data = wisc_pred) |> 
  step_normalize(all_numeric()) |> 
 step_pca(all_numeric(), num_comp = 2) #|>
#  prep() |>
 # bake(new_data = NULL)

workflow_km <- workflow() |> 
  add_model(kmeans_spec) |>
  add_recipe(kmeans_rec)

kmeans_fit <-  workflow_km |> 
  fit(wisc_pred)

wisc_km <- augment(kmeans_fit, new_data = wisc_pred) 

fviz_cluster(list(data = scale(wisc_pred), cluster = wisc_class$Diagnosis),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(wisc_class$Diagnosis, wisc_km$.pred_cluster)
```

4.  Choose either the complete set of variables or the reduced set of principal components, and perform hyperparameter tuning to find the optimal value of k in k-means using `tidymodels` tools. Make a plot of within SSE (sum of squared errors within clusters) as a function of k and use the elbow method to determine the optimal value of k.

```{r}
kmeans_spec_tuned <- kmeans_spec |> 
  set_args(num_clusters = tune())


pca_recipe <- 
  recipe(formula =  ~ ., data = wisc_pred) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) |>
  step_pca(all_predictors(), threshold = 0.9) 

kmeans_wf <- workflow() |>
  add_model(kmeans_spec_tuned) |>
  add_recipe(pca_recipe)

splits <- bootstraps(wisc_pred, times = 10)

num_clusters_grid <- tibble(num_clusters = seq(1, 10))

tune_res <- tune_cluster(
  object = kmeans_wf,
  resamples = splits,
  grid = num_clusters_grid
)

tune_res |>
  collect_metrics()

tune_res |>
  autoplot()
```

5.  Compare the performance of clustering with and without reducing dimensionality, in terms of its agreement with the true labels (from Diagnosis). Explain the meaning of the elbow method for hyperparameter tuning and whether the result is surprising.

K-means does a better job with reduced dimensions (2 PCs) than with the full data set; the total number of mislabeled points goes down. From the elbow plot, it appears that 2 is the optimal number of clusters; the elbow method essentially assumes that at the optimal number of clusters the within cluster variance decreases the fastest and for larger k we see diminishing returns.

## Dimensionality reduction of neuroblastoma data

The following data set is gene expression data from tumors of patients with neuroblastoma (a type of cancer), accession number [**GSE62564**](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE62564). It contains 22 phenotypic scores, 6 of which (MYCN, STC1, P4HA1, BHLHE40, HIF1A and ST8SIA1) are gene expressions measured in log2RPM (log 2 reads per million). The other 16 are quantified by R package GSVA (Gene Set Enrichment Analysis).

```{r}
neuro_blast <- read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/r2_gse62564_GSVA_Metadata_selected.csv")
```

1.  Clean the data to remove any outliers or missing values, and select only the predictor variables for dimensionality reduction, and leave `high risk` in a separate tibble. Perform PCA on the data (after scaling the variables), and make a Scree plot of the eigenvalues. Plot the data points projected onto the first principal components colored by `high risk` value.

```{r}
neuro_pred <- neuro_blast |>  drop_na() |>
  dplyr::select(-c(high_risk, `sample id`))

neuro_risk <- neuro_blast |>  dplyr::select(high_risk) |> 
  drop_na() |>
  mutate(high_risk = factor(high_risk))


neuro_pca <- neuro_pred  |> 
  prcomp(scale = TRUE)


tidy(neuro_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
  ggplot(aes(PC, percent)) +
  geom_col()

fviz_pca_ind(neuro_pca,
             col.ind = factor(neuro_risk$high_risk),
             label = 'none',
             addEllipses=TRUE, 
             ellipse.level=0.9, 
             palette = "Dark2")


```

2.  Perform k-means clustering with k=2 on this data set using a few of the PCs. Make a plot of all the data points colored by their assigned cluster label. Print out a confusion matrix of the assigned cluster labels with the `high risk` value for all the points.

```{r}

neuro_rec <- neuro_pred |> 
  recipe(formula =  ~ .) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) |>
  step_pca(all_predictors(), num_comp = 2) 

kmeans_wf <- workflow() |>
  add_model(kmeans_spec) |>
  add_recipe(neuro_rec)

kmeans_fit <- kmeans_wf  |> 
  fit(neuro_pred)

neuro_km <- predict(kmeans_fit, new_data = neuro_pred)

fviz_cluster(list(data = scale(neuro_pred), cluster = neuro_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(neuro_km$.pred_cluster, neuro_risk$high_risk)

```

3.  Perform hierarchical clustering on the same data set using the method "single".

```{r}
hclust_spec <- hier_clust(linkage_method = "single")
  

hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(neuro_rec)

hclust_fit <- hclust_wf  |> 
  fit(neuro_pred)

neuro_hc <- predict(hclust_fit, new_data = neuro_pred, num_clusters = 2)

fviz_cluster(list(data = scale(neuro_pred), cluster = neuro_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using single linkage method:")
table(neuro_hc$.pred_cluster, neuro_risk$high_risk)
```

4.  Try using other linkage methods (e.g. "complete" and "average") and report the same confusion matrix for both.

```{r}

hclust_spec <- hier_clust(linkage_method = "complete")
  

hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(neuro_rec)

hclust_fit <- hclust_wf  |> 
  fit(neuro_pred)

neuro_hc <- predict(hclust_fit, new_data = neuro_pred, num_clusters = 2)

fviz_cluster(list(data = scale(neuro_pred), cluster = neuro_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using average linkage method:")
table(neuro_hc$.pred_cluster, neuro_risk$high_risk)
```

```{r}

hclust_spec <- hier_clust(linkage_method = "average")
  

hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(neuro_rec)

hclust_fit <- hclust_wf  |> 
  fit(neuro_pred)

neuro_hc <- predict(hclust_fit, new_data = neuro_pred, num_clusters = 2)

fviz_cluster(list(data = scale(neuro_pred), cluster = neuro_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using average linkage method:")
table(neuro_hc$.pred_cluster, neuro_risk$high_risk)
```

5.  Report how the clustering quality (as judged by comparison with the true value of risk) depends on the type of clustering algorithm. Discuss how the relative performance of the methods depends on the geometry of the point distribution you observed in the plots.

The clustering quality is better for the k-means method than any of the hierarchical methods, but all of the do poorly compared to the clustering on the Wisconsin data. The reason is likely due to the categories not being well separated in the plots, and the fact that the actual clusters are fairly spherical, so k-means is more appropriate than agglomerative methods.

```{r}
data("hepatic_injury_qsar", package = "modeldata")
#glimpse(hepatic_injury_qsar)
```

```{r}
liver_data <- hepatic_injury_qsar |> drop_na() |> dplyr::select(-class)

liver_class <- hepatic_injury_qsar |>  dplyr::select(class) |> 
  drop_na() |>
  mutate(class = factor(class))


liver_pca <- liver_data  |> 
  prcomp(scale = TRUE)


tidy(liver_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
  ggplot(aes(PC, percent)) +
  geom_col()

fviz_pca_ind(liver_pca,
             col.ind = factor(liver_class$class),
             label = 'none',
             addEllipses=TRUE, 
             ellipse.level=0.9, 
             palette = "Dark2")

```

Identify the variables that have zero variance and remove them

```{r}
zerovar <- hepatic_injury_qsar |>  dplyr::select(-class) |> 
  summarise_all(var) |>  
  select_if(function(.) . == 0) |> 
  names()

liver_clean <- hepatic_injury_qsar |> dplyr::select(-class, -zerovar) |> 
  drop_na()
  
liver_pca <- liver_clean  |> 
  prcomp(scale = TRUE)


tidy(liver_pca, matrix = "eigenvalues") |> filter(PC < 6) |> 
  ggplot(aes(PC, percent)) +
  geom_col()

fviz_pca_ind(liver_pca,
             col.ind = factor(liver_class$class),
             label = 'none',
             addEllipses=TRUE, 
             ellipse.level=0.9, 
             palette = "Dark2")
```

```{r}
liver_rec <- liver_clean |> 
  recipe(formula =  ~ .) |> 
  step_novel(all_nominal_predictors()) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_normalize(all_predictors()) #|>
#  step_pca(all_predictors(), num_comp = 2) 

kmeans_spec <- k_means(num_clusters = 3) |>
  set_engine("stats")

kmeans_wf <- workflow() |>
  add_model(kmeans_spec) |>
  add_recipe(liver_rec)

kmeans_fit <- kmeans_wf  |> 
  fit(liver_clean)

liver_km <- predict(kmeans_fit, new_data = liver_clean)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_km$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

table(liver_km$.pred_cluster, liver_class$class)
```

```{r}
hclust_spec <- hier_clust(linkage_method = "complete")
  

hclust_wf <- workflow() |>
  add_model(hclust_spec) |>
  add_recipe(liver_rec)

hclust_fit <- hclust_wf  |> 
  fit(liver_clean)

liver_hc <- predict(hclust_fit, new_data = liver_clean, num_clusters = 3)

fviz_cluster(list(data = scale(liver_clean), cluster = liver_hc$.pred_cluster),
ellipse.type = "norm", geom = "point", stand = FALSE, palette = "jco", ggtheme = theme_classic())

print("The confusion matrix for hierarchical clustering using single linkage method:")
table(liver_hc$.pred_cluster, liver_class$class)
```
