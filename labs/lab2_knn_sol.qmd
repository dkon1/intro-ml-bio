---
title: "KNN and bias-variance tradeoff (lab 2 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to demonstrate fundamental concepts of machine learning, such as error in training and test sets, bias-variance tradeoff, using KNN regression and classification. Here is what you will do:

1.  Clean a given data set by removing missing values and outliers and selecting the variables you want to work with.

2.  Apply knn method either for classification or regression on a training set and validate on a test set.

3.  Report the error of the classification or regression on both training and test sets.

4.  Repeat the process for different number of nearest neighbors (hyperparameter k) and compare the results.

```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(class)
library(FNN)
```

## Heart rates

The following data set contains heart rates measured and reported by students in my class Introduction to Quantitative Modeling for Biology. There are four different heart rates measured (two at rest and two after exercise) and the year it was measured.

```{r}
heart_rates <- read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/HR_data_combined.csv")
```

1.  Select a response and an explanatory variable and clean the data to remove any outliers or missing values in these variables. Split the data set into training and test sets.

    ```{r}
    heart_data <- heart_rates |> 
      dplyr::select(Rest1, Ex1) |> 
      drop_na() #|> 
    # arrange(Rest2) 


    train_index <- sample(nrow(heart_data), size = floor(0.5 * nrow(heart_data)))

    heart_train <- heart_data %>% 
      slice(train_index) %>% 
      arrange(Rest1)

    heart_test <- heart_data %>% 
      slice(-train_index) |> 
      arrange(Rest1) 

    X_train <- heart_train %>% 
      dplyr::select(Rest1) 
    X_test <- heart_test %>% 
      dplyr::select(Rest1) 
    Y_train <- heart_train %>% 
      dplyr::select(Ex1) 
    Y_test <- heart_test %>% 
      dplyr::select(Ex1)
    ```

2.  Make a prediction for the test set using knn regression with k=1, and plot the predicted values over the actual data for the test set. Report the variance of the residuals for the test set.

```{r}
num = 1
heart_pred <- knn.reg(train = X_train, test = X_test, y=Y_train$Ex1,  k = num)
# base R:
#plot(X_test$Rest1, Y_test$Ex1, cex = .8, col = "blue", main = paste("KNN regression with k =", num))
#lines(X_test$Rest1, heart_pred$pred, col = "darkorange", lwd = 2)
# ggplot:
 ggplot() + 
  aes(x = X_test$Rest1, y = Y_test$Ex1) + geom_point(color = 'blue') +
  geom_line(aes(x = X_test$Rest1, y = heart_pred$pred), color = 'darkorange') + ggtitle(paste("KNN regression with k =", num))
 
 print(paste("sum of squared errors for the test set:", 
             sum((Y_test$Ex1-heart_pred$pred)^2)))
```

3.  Use knn regression with k=5 and the training set the same as the test set, and plot the predicted values over the actual data for the training set. Report the variance of the residuals for the test set.

```{r}
num = 5
heart_pred <- knn.reg(train = X_train, test = X_train, y=Y_train$Ex1,  k = num)
# base R:
plot(X_train$Rest1, Y_train$Ex1, cex = .8, col = "blue", main = paste("KNN regression with k =", num))
lines(X_train$Rest1, heart_pred$pred, col = "darkorange", lwd = 2)
# ggplot:
 ggplot() + 
  aes(x = X_train$Rest1, y = Y_train$Ex1) + geom_point(color = 'blue') +
  geom_line(aes(x = X_train$Rest1, y = heart_pred$pred), color = 'darkorange') + ggtitle(paste("KNN regression with k =", num))
 
print(paste("sum of squared errors for the test set:", 
             sum((Y_test$Ex1-heart_pred$pred)^2)))
```

4.  Repeat knn regression on the test and training sets for a range of k, both smaller and larger than 5 (you can use a loop or write a function and use `replicate`.) For each k, calculate the variance of the residuals on the test set and on the training set, and assign each to a vector. Make a plot of the variance of the errors for the test set and for the training set as a function of k as two lines of different colors and add a legend.

```{r}
# base R
ks <- 1:100
test_err<- c()
train_err<- c()
for (num in ks) {
  heart_pred <- knn.reg(train = X_train, test = X_test, y=Y_train$Ex1,  k = num)
  test_err <- c(test_err, sum((heart_pred$pred - Y_test$Ex1)^2))
  heart_pred <- knn.reg(train = X_train, test = X_train, y=Y_train$Ex1,  k = num)
  train_err <- c(train_err, sum((heart_pred$pred - Y_train$Ex1)^2))
}

plot(ks, train_err, type = 'l', main = "SSE for different k")
lines(ks, test_err, col = 'red')
legend("topright", c("train", "test"), col=c(1,2), lty = 1)
```

5.  What seems to be the optimal number of nearest neighbors? Explain how you see bias-variance tradeoff playing out in this example.

The error for both test and training scores is highest for k=1, for larger k it decreases and then increases; but the training set has lower error for smaller k, and then for k\>30 it has higher error than the test set. The optimal value of k is around 20-30.

### Neuroblastoma data

The following data set is gene expression data from tumors of patients with neuroblastoma (a type of cancer), accession number [**GSE62564**](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE62564)**.** It contains 22 phenotypic scores, 6 of which (MYCN, STC1, P4HA1, BHLHE40, HIF1A and ST8SIA1) are gene expressions measured in log2RPM (log 2 reads per million). The other 16 are quantified by R package GSVA (Gene Set Enrichment Analysis).

```{r}
neuro_blast <- read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/r2_gse62564_GSVA_Metadata_selected.csv")
```

1.  Clean the data to remove any outliers or missing values in these variables, and select all the numeric variables. Split the data set into training and test sets.

    ```{r}
    neuro_data <- neuro_blast |> dplyr::select(-c(`sample id`, high_risk)) |> drop_na()
    train_index <- sample(nrow(neuro_data), size = floor(0.5 * nrow(neuro_data)))
    neuro_train <- neuro_data[train_index, ]
    neuro_test <- neuro_data[-train_index, ]
    risk_train <- neuro_blast$high_risk[train_index]
    risk_test <- neuro_blast$high_risk[-train_index]
    ```

2.  Use the `knn` function from package `class` to predict the risk status (`high_risk` response variable) for the test set using k=5.

```{r}
# base R:
knn_out <- knn(train = neuro_train, 
         test = neuro_test, 
         cl = risk_train, 
         k = 5)
```

3.  Compute the accuracy of knn classification of high_risk, by printing the table of true vs predicted classes (confusion matrix) as well as the accuracy (fraction of agreement between true and predicted classes out of all predictions).

```{r}
# base R
print(table(knn_out, risk_test))
print(sum(knn_out==risk_test)/length(knn_out))
```

4.  Repeat the classification for a range of values of k, both smaller and larger than 5; calculate the accuracy both for the test set and the training set and assign them as vectors. Plot the resulting accuracy scores as functions of k with different colors, and add a legend.

```{r}
ks <- 1:20
acc_train <- c()
acc_test <- c()
for (i in ks) {
  knn_out <- knn(train = neuro_train, 
         test = neuro_test, 
         cl = risk_train, 
         k = i)
  print(table(knn_out, risk_test))
  acc_test <- c(acc_test,sum(knn_out==risk_test)/length(knn_out))
  knn_out <- knn(train = neuro_train, 
         test = neuro_train, 
         cl = risk_train, 
         k = i)
  print(table(knn_out, risk_train))
  acc_train <- c(acc_train,sum(knn_out==risk_train)/length(knn_out))
}

plot(ks, acc_test, type = 'l', ylim = c(0.8,1), main = "Mean classification accuracy for different k")
lines(ks, acc_train, col = 'red')
legend("topright", c("test", "train"), col=c(1,2), lty = 1)
```

5.  What seems to be the optimal number of nearest neighbors? Explain how you see bias-variance tradeoff playing out in this example.

The complexity of the model decreases as k increases, and the accuracy (the opposite of error) first increases and then goes down. There is an optimal number of nearest neighbors which seems to be 3.
