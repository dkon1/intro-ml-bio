---
title: "Resampling: cross-validation and bootstrap (lab 4 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to use resampling methods for cross-validation and performing statistical inference. You will learn to do the following:

1.  use k-fold validation for estimation of testing error

2.  use bootstrap to calculate confidence intervals for parameters

3.  use bootstrap to calculate p-values

4.  explain the meaning of these results

The use of these models is demonstrated in the week 5 tutorials using the tools from package `tidymodels`; I recommend that you use them to perform the tasks below.

```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(discrim)
```

## Classification using k-fold validation on the Neuroblastoma data

The following data set is gene expression data from tumors of patients with neuroblastoma (a type of cancer), accession number [**GSE62564**](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE62564)**.** It contains 22 phenotypic scores, 6 of which (MYCN, STC1, P4HA1, BHLHE40, HIF1A and ST8SIA1) are gene expressions measured in log2RPM (log 2 reads per million). The other 16 are quantified by R package GSVA (Gene Set Enrichment Analysis).

```{r}
neuro_blast <- read_csv("https://raw.githubusercontent.com/dkon1/intro-ml-bio/main/labs/data/r2_gse62564_GSVA_Metadata_selected.csv")
```

1.  Clean the data to remove any missing values and convert the . Set up a tidymodels recipe to predict the variable `high_risk` (make sure to convert it to a factor!) using all other variables, except for `sample_id.` Define a *generalized linear model* (using function \`logistic_reg\` and engine "glm") specification to predict the variable `high_risk`, and create a workflow by combining the recipe and the model spec. Use the function `vfold_cv` to split the cleaned data set, then `fit_resamples` to fit the model on different splits and perform k-fold cross-validation, then use the function `collect_metrics` to validate the model.

    ```{r}
    neuro_clean <- neuro_blast |>  drop_na() |>
      mutate(high_risk = factor(high_risk))


    neuro_recipe <- 
      recipe(high_risk ~ ., data = neuro_clean) |>  
      update_role(`sample id`, new_role = "ID")


    glm_spec <- 
        logistic_reg() |>  
        set_engine("glm")

    workflow_glm <- workflow() |> 
      add_model(glm_spec) |> 
      add_recipe(neuro_recipe)

    folds <- vfold_cv(neuro_clean, v = 10)

    glm_fit <- 
       workflow_glm |>  
      fit_resamples(folds)

    print("The metrics for the GLM classification are:")
    collect_metrics(glm_fit)
    ```

2.  Repeat this process for Naive Bayes and use the function `collect_metrics` to validate the model.

    ```{r}
    nb_spec <- naive_Bayes() |>  
      set_mode("classification") |>  
      set_engine("naivebayes") |>  
      set_args(usekernel = FALSE) 

    workflow_nb <- workflow() |> 
      add_model(nb_spec) |> 
      add_recipe(neuro_recipe)

    folds <- vfold_cv(neuro_clean, v = 10)

    nb_fit <- 
       workflow_nb |>  
      fit_resamples(folds)

    print("The metrics for the Naive Bayes model are:")
    print(collect_metrics(nb_fit))
    ```

3.  Repeat this process for LDA and use the function `collect_metrics` to validate the model.

    ```{r}
    ld_spec <- discrim_linear() |> 
      set_mode("classification") |>  
      set_engine("MASS") |>  
      set_args(penalty = NULL, regularization_method = NULL) 


    workflow_ld <- workflow() |> 
      add_model(ld_spec) |> 
      add_recipe(neuro_recipe)

    folds <- vfold_cv(neuro_clean, v = 10)

    ld_fit <- 
       workflow_ld |>  
      fit_resamples(folds)

    print("The error stats for the LDA model are:")
    print(collect_metrics(ld_fit))
    ```

4.  Repeat this process for QDA and use the function `collect_metrics` to validate the model.

```{r}
qd_spec <- discrim_quad()  |> 
  set_mode("classification") |>  
  set_engine("MASS") |>  
  set_args(penalty = NULL, regularization_method = NULL) 


workflow_qd <- workflow() |> 
  add_model(qd_spec) |> 
  add_recipe(neuro_recipe)


folds <- vfold_cv(neuro_clean, v = 10)

qd_fit <- 
   workflow_qd |>  
  fit_resamples(folds)

print("The error stats for the QDA model are:")
print(collect_metrics(qd_fit))

```

5.  Compare the results from the four classification methods both in terms of the accuracy and the ROC area under the curve metric; explain which method is best for this data set.

The accuracy and the roc_auc tell the same story: accuracy is highest for GLM (88%), is about the same for LDA and QDA (86%) (but since QDA is more complex, LDA is preferred) and lowest for NB (83%). Thus, logistic regression performs best on this data set.

## Parameter estimation using the bootstrap: Ecological data

The following data set contains observations of the populations of one species of fish (cutthroat trout) and two species of salamander in Mack Creek, Andrews Forest, Willamette National Forest, Oregon. The data set contains 16 variables and over thirty-two thousand observations. The variables include time and date, location, and measurements, such as size and weight. The metadata (descriptions of data) are provided [here](https://portal.edirepository.org/nis/metadataviewer?packageid=knb-lter-and.4027.14) (click on "Data entities" tab for explanations of each variable.)

```{r}
mack_data <- read_csv("https://raw.githubusercontent.com/dkon1/quant_life_quarto/main/data/mack_data.csv")
```

1.  Clean the data to remove any outliers or missing values and filter the data to contain observations from only one species, either: 'Cutthroat trout' or 'Coastal giant salamander'. Use the function `bootstraps`to create many (\>1000) random splits of the cleaned data set; then define a new function to perform linear regression on a split, to predict the variable `weight_g` using the variable `length_1_mm` .

    ```{r}
    mack_clean <- mack_data |>
      filter(species == 'Coastal giant salamander') |> 
      drop_na(weight_g, length_1_mm)
     

    boots <- bootstraps(mack_clean, times = 2000, apparent = TRUE)

    lm_bootstrap <- function(split) {
        lm(weight_g ~ length_1_mm, analysis(split))
    }

    lm_boot_models <-
      boots |> 
      mutate(model = map(splits, lm_bootstrap),
             coef_info = map(model, tidy))


    #boot_aug <- 
    #  boot_models |>  
    #  sample_n(100) |>  
    #  mutate(augmented = map(model, augment)) |> 
    #  unnest(augmented)

    #boot_aug |> ggplot( aes(x = length_1_mm, y = weight_g )) +
    #  geom_line(aes(y = .fitted, group = id), alpha = .2, col = "blue") +
    #  geom_point()


    ```

2.  Use the example from the resampling tutorial to train the model on all the random splits, and calculate and print the (95%) confidence intervals for the coefficients of linear regression based on the bootstrap calculation.

    ```{r}
    boot_coefs <- 
      lm_boot_models |>  
      unnest(coef_info)

    percentile_intervals <- int_pctl(lm_boot_models, coef_info)
    percentile_intervals
    ```

3.  Repeat the same process as in question 1, but by training a quadratic regression model on the bootsrapped splits of the data set; for this you'll need to define a new auxiliary function with a quadratic model, then calculate and print out the parameters of the quadratic regression from the bootstrap calculation.\

```{r}
quad_bootstrap <- function(split) {
    lm(weight_g ~ poly(length_1_mm, degree = 2, raw = TRUE), analysis(split))
}

quad_boot_models <-
  boots %>% 
  mutate(model = map(splits, quad_bootstrap),
         coef_info = map(model, tidy))

quad_boot_coefs <- 
  quad_boot_models %>% 
  unnest(coef_info)


percentile_intervals <- int_pctl(quad_boot_models, coef_info)
percentile_intervals
```

3.  Perform linear regression using `lm`() on the cleaned mack data set with the same variables, and print out the confidence intervals (at 95% level) for the two coefficients from the calculated statistics (you can use the function `confint`).

    ```{r}
    lm_out <- lm(weight_g ~ length_1_mm, data = mack_clean)
    summary(lm_out)

    print(confint(lm_out, "(Intercept)", level=0.95))
    print(confint(lm_out, "length_1_mm", level=0.95))
    ```

4.  Perform quadratic regression using `lm` on the same variables and again print out the confidence intervals for the three coefficients.

```{r}
quad_out <- lm(weight_g ~ poly(length_1_mm, degree = 2, raw = TRUE), data = mack_clean)
summary(quad_out)

print(confint(quad_out, "(Intercept)", level=0.95))
print(confint(quad_out, "poly(length_1_mm, degree = 2, raw = TRUE)1", level=0.95))
print(confint(quad_out, "poly(length_1_mm, degree = 2, raw = TRUE)2", level=0.95))
```

5.  Compare the estimates for the parameters of the linear regression and quadratic regression between the bootstrap version and the classical linear regression statistics.

The point estimates are similar, but the bootstrapped confidence intervals are consistently wider (especially in the quadratic case). This is typical for using a non-parametric empirical distribution compared to a theoretical distribution (e.g. for the t-statistic) because it doesn't have the same computational power.

## Hypothesis testing using the bootstrap

Let us use the awesome power of the bootstrap to construct an empirical distribution for a null hypothesis.

1.  Use the ecological data set and the pipeline for calculating statistics from the tutorial resampling tutorial to calculate the mean difference between the weight of trout by section "CC" and "OG". Use the same tools to generate a null distribution for the difference in means between the weight of trout in the two sections (use `hypothesize` with `null="independence"` and `generate` with the `type = "permute"` option.)

    ```{r}
    null_dist <-mack_clean |> 
      specify(weight_g   ~ section) |> 
      hypothesize(null = "independence") |> 
      generate(reps = 500, type = "permute") |> 
      calculate(stat="diff in means", order = c("CC", "OG"))

    d_hat <- mack_clean |> 
      specify(weight_g  ~ section) |> 
      calculate(stat="diff in means", order = c("CC", "OG"))
    ```

2.  Use the function `visualize` and `shade_p_value` with the mean difference between trout, to show how the observation compares to the null distribution.

```{r}
null_dist |> 
  visualize()  +
  shade_p_value(obs_stat = d_hat, direction = "right")
```

3.  Estimate the p-value from the histogram by counting the fraction of the histogram that is greater than the observed statistic.

The observed statistic is outside of any stats generated by bootstrapping, so the p-value must be lower than 1/500, or 0.002.

4.  Use the same tools with the neuroblastoma data to test whether there is a difference in the variable `STC1` between high and low risk samples. Use the same tools to generate a null distribution for the difference in means between the weight of trout in the two sections (use `hypothesize` with `null="independence"` and `generate` with the `type = "permute"` option.)

```{r}
null_dist <- neuro_clean |> 
  specify(STC1 ~ high_risk ) |> 
  hypothesize(null = "independence") |> 
  generate(reps = 500, type = "permute") |> 
  calculate(stat="diff in means", order = c('yes', 'no'))

d_hat <- neuro_clean |> 
  specify(STC1 ~ high_risk ) |> 
  calculate(stat="diff in means", order = c('yes', 'no'))

```

5.  Use the function `visualize` and `shade_p_value` with the mean difference between expression values of this gene to show how the observation compares to the null distribution.

```{r}
null_dist |> 
  visualize()  +
  shade_p_value(obs_stat = d_hat, direction = "right")
```

6.  Estimate the p-value from the histogram by counting the fraction of the histogram that is greater than the observed statistic.

    The weight of the histogram shaded in pink is about 40 out of 500, or about 0.08.

7.  Use the function `t.test` to calculate and print the p-value for the null hypothesis of equal weights of trout for both sections.

```{r}
result <- t.test(weight_g  ~ section, data = mack_clean)
print(result)

```

8.  Do the same for the null hypothesis of equal gene expression values of `STC1` for high and low risk samples.

```{r}
result <- t.test(STC1 ~ high_risk, data = neuro_clean)
print(result)
```

9.  Compare the p-values produced by the bootstrap calculation and the standard statistical test for the comparison of trout between the two forest sections. Are the conclusions consistent between the two methods?

The p-values are both extremely low, so in both cases the conclusion is to reject the null.

10. Compare the p-values produced by the bootstrap calculation and the standard statistical test for the comparison of gene expression values between high and low risk neuroblastoma data. Are the conclusions consistent between the two methods?

The bootstrapped p-value is about 0.08, while the t-test p-value is about 0.17. In both cases, one would not reject the null hypothesis for any reasonable significance level.
