---
title: "Resampling: cross-validation and bootstrap (lab 5 for BIOS 26122)"
author: "Dmitry Kondrashov"
format: 
  html:
    self-contained: true
editor: visual
---

## Description

The goal of this assignment is to use selection and methods for cross-validation and performing statistical inference. You will learn to do the following:

The use of these models is demonstrated in the week 5 tutorials using the tools from package `tidymodels`; I recommend that you use them to perform the tasks below.

```{r setup}
#| include: false
#| echo: false
library(tidyverse)
library(tidymodels)
library(discrim)
library(faraway)
library(ISLR2)
```

###  Using ridge regression

### 

```{r}
#lowbwt <- read_csv("data/lowbwt.csv")
diabetes <- read_tsv("https://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt")

data("Hitters")
```

1.  Clean the data to remove any outliers or missing values, and filter the data to contain observations from only species, either: 'Cutthroat trout' or 'Coastal giant salamander' . Split the data set into training and test sets of equal size. Set up a `tidymodels` recipe to predict the variable \`section\` (make sure it's converted to a factor) using the numeric variables `length_1_mm` and `weight_g`.

    ```{r}
    Hitters_clean <- Hitters |> 
     #dplyr::select(-c(League, Division, NewLeague)) |> 
     drop_na()


    # Put 3/4 of the data into the training set 
    Hitters_split <- initial_split(Hitters_clean, prop = 0.5)

    # Create data frames for the two sets:
    Hitters_train <- training(Hitters_split)
    Hitters_test  <- testing(Hitters_split)


    Hitters_recipe <- 
      recipe(Salary ~ ., data = Hitters_train) #|> 
    #  update_role(section, reach, new_role = "ID")
    ```

2.  Train a *generalized linear model* (using engine "glm") to predict the variable `section`; specify the model, create a workflow, fit it to the training set, print out the fitted parameters, and evaluate its performance on the test set.

```{r}
ridge_spec <- linear_reg(mixture = 0, penalty= 0) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

workflow_ridge <- workflow() |> 
  add_model(ridge_spec) |> 
  add_recipe(Hitters_recipe)

fit_ridge <- workflow_ridge |> 
  fit(Hitters_train)

tidy(fit_ridge)

#tidy(fit_ridge) |>  
#  dwplot(dot_args = list(size = 2, color = "black"),
#         whisker_args = list(color = "black"),
#         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))

compare_pred <- augment(fit_ridge, new_data = Hitters_test) 


compare_pred |> rmse(Salary, .pred)

```

Try different values of penalty:

```{r}
ridge_spec <- linear_reg(mixture = 0, penalty = 10) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

workflow_ridge <- workflow() |> 
  add_model(ridge_spec) |> 
  add_recipe(Hitters_recipe)


fit_ridge <- workflow_ridge |> 
  fit(Hitters_train)

tidy(fit_ridge, penalty= 10)

#tidy(fit_ridge) %>% 
#  dwplot(dot_args = list(size = 2, color = "black"),
#         whisker_args = list(color = "black"),
#         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))

compare_pred <- augment(fit_ridge, new_data = Hitters_test) 


compare_pred |> rmse(Salary, .pred)
```

3.  Use the tools of parameter tuning to select the best penaltyvalue lambda

    ```{r}

    Hitters_fold <- vfold_cv(Hitters_train, v = 10)

    ridge_recipe <- 
      recipe(Salary~ ., data = Hitters_train) |> 
      step_novel(all_nominal_predictors()) |>  
      step_dummy(all_nominal_predictors()) |>  
      step_zv(all_predictors()) |> 
      step_normalize(all_predictors())

    ridge_spec <- 
      linear_reg(penalty= tune(), mixture = 1) |> 
      set_mode("regression") |> 
      set_engine("glmnet")

    ridge_workflow <- workflow() |> 
      add_recipe(ridge_recipe) |> 
      add_model(ridge_spec)


    penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)



    tune_res <- tune_grid(
      ridge_workflow,
      resamples = Hitters_fold, 
      grid = penalty_grid
    )
    ```

4.  Plot the results and print out the best value of the penaltyparameter

    ```{r}
    autoplot(tune_res)

    collect_metrics(tune_res)

    best_penalty<- select_best(tune_res, metric = "rmse")

    best_penalty


    ```

5.  Validate the final model with the best fit value of lambda and compare the results to those from the (penalty=0) fit

```{r}
 
ridge_final <- finalize_workflow(ridge_workflow, best_penalty)

ridge_final_fit <- fit(ridge_final, data = Hitters_train)

augment(ridge_final_fit, new_data = Hitters_test) |>
  rmse(truth = Salary, estimate = .pred)

tidy(ridge_final_fit, penalty= best_penalty$penalty)
```

```{r}
grid <- 10^seq(10, -2, length = 100)
x_train <- model.matrix(Salary ~ ., Hitters_train)[, -1]
x_test <- model.matrix(Salary ~ ., Hitters_test)[, -1]

lasso.mod <- glmnet(x_train, Hitters_train$Salary, alpha = 1,
    lambda = grid)
plot(lasso.mod)
###
#set.seed(1)
cv.out <- cv.glmnet(x_train, Hitters_train$Salary, alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam,
    newx = x_test)
print(paste("Root mean squared error:", sqrt(mean((lasso.pred - Hitters_test$Salary)^2))))
print(paste("correlation:", cor(lasso.pred,Hitters_test$Salary)))
###
out <- glmnet(model.matrix(Salary ~ ., Hitters), Hitters$Salary, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients",
    s = bestlam)[1:20, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```
