---
title: "Lab 8: Multiple hypothesis testing and false discovery rate"
format: 
  html:
    self-contained: true
editor: visual
---

```{r}
#| include: false
library(tidyverse)

```

## Description

The goal of this assignment is to simulate multiple hypothesis testing, so that we can calculate the testing error rates for different scenarios. You will learn the following:

1.  How to compute Type 1 and Type 2 errors.
2.  Compute the Family-Wide Error Rate and the False Discovery Rate.
3.  Use different p-value correction methods.
4.  Simulate testing a mix of true and false hypotheses.
5.  Observe the impact of effect size and prior probability on the false discovery rate.

## Part 1: simulations of hypothesis testing

In this part you will simulate a hypothesis testing for independence. The chunk below contains a function that generates a fake data set with two different variables: genotype (either A or B) and health status ('D' or 'H'). The input arguments control the true (or population) probability of disease for genotype A, same for genotype B, and the number of individuals in both sample groups.

```{r}
gen_ind_test <- function(probA, probB, samp_size) {
  health_states <- c('D', 'H') # health states 'D' and 'H'
  dis_genA <- sample(health_states, samp_size, replace = TRUE, prob = c(probA, 1-probA)) # generate a vector of health states for genotype A
  dis_genB <- sample(health_states, samp_size, replace = TRUE, prob = c(probB, 1-probB)) # generate a vector of health states for genotype B
  data_vec <- c(table(dis_genA), table(dis_genB)) # calculate number of H and D for each genotype
  data_mat <- matrix(data_vec, nrow=2, ncol=2) # put those together into a data matrix
  chisq_result <- chisq.test(data_mat) # run chi-squared test
  return(chisq_result$p.value) # output the p-value
} 
```

1.  Simulate 1000 hypothesis tests with *identical* probabilities of disease (e.g. 0.2) and a sample size of 200. Report the number of null hypothesis rejections at significance level alpha = 0.01 and the number of errors. What type of errors do you observe? Report the error rate.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200
pvals <- replicate(1000, gen_ind_test(probA, probB, samp_size))
#plot(-log(pvals), type = 'h')
alpha <- 0.01
print("Fraction of rejections at alpha = 0.01:")
print(sum(pvals < alpha)/1000)
print("The False Positive Rate is:")
print((sum(pvals < alpha))/1000)
```

2.  Calculate the FWER (family-wise error rate) for a simulation of 10 repeated hypothesis tests with equal probabilities of disease and sample size 200 (as in question 1) by using a for loop or making a function and then using replicate to repeat the simulation 100 times, and report the fraction of times that *at least one of the hypotheses was rejected at significance level 1*. Repeat the calculation for 50, 100, and 500 hypothesis tests and report the FWER as a function the number of hypotheses tested.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
alpha <- 0.01
m <- 10 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))

  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))

m <- 50 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))

  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 100 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))

  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 500 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))

  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))

```

3.  Use the Bonferroni correction for the p-values and repeat the calculation in question 2: report the FWER for repeated testing of 10, 50, 100, and 500 true null hypotheses.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
alpha <- 0.01
m <- 10 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "bonferroni")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))

m <- 50 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "bonferroni")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 100 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "bonferroni")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 500 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "bonferroni")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))

```

4.  Use the Holm correction for the p-values and repeat the calculation in question 3: report the FWER for repeated testing of 10, 50, 100, and 500 true null hypotheses.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
alpha <- 0.01
m <- 10 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "holm")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))

m <- 50 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "holm")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 100 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "holm")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))


m <- 500 # number of hypotheses
atleast1 <- 0
for (i in 1:100) {
  pvals <- replicate(m, gen_ind_test(probA, probB, samp_size))
  pvals <- p.adjust(pvals, method = "holm")
  if (sum(pvals < alpha) >0) {
      atleast1 <- atleast1 + 1    
  }
}
print(paste("The FWER for", m," hypothesis tests is:",atleast1/100))
```

## Part 2: False Discovery Rate

5.  Simulate 1000 hypothesis tests with probabilities of disease of 0.1 (for genotype A) and 0.2 (for genotype B) and a sample size of 200 - we'll call this a *small effect size*. Report the number of null hypothesis rejections at significance level alpha = 0.01 and the number of errors. What type of errors do you observe? Report the error rate. Increase the probability of disease of genotype B to 0.4 - we'll call this a *large effect size* - and again report the error rate.

```{r}
probA <- 0.1
probB <- 0.2
samp_size <- 200 # sample size
pvals <- replicate(1000, gen_ind_test(probA, probB, samp_size))
#plot(-log(pvals), type = 'h')
alpha <- 0.01
print("Fraction of rejections at alpha = 0.01:")
print(sum(pvals < alpha)/1000)
print("The False Negative Rate for a small effect is:")
print(1-(sum(pvals < alpha))/1000)

probA <- 0.1
probB <- 0.4
pvals <- replicate(1000, gen_ind_test(probA, probB, samp_size))
#plot(-log(pvals), type = 'h')
alpha <- 0.01
print("Fraction of rejections at alpha = 0.01:")
print(sum(pvals < alpha)/1000)
print("The False Negative Rate for a large effect is:")
print(1-(sum(pvals < alpha))/1000)
```

6.  Let us simulate a scenario where some fraction of the hypotheses are true (equal probabilities of disease) and some are false (different probabilities). Let us call the fraction of true hypotheses our prior. Set the prior probability to be 90%, and simulate 1000 hypotheses: 90% true and 10% false, with a small effect (probabilities of 0.1 and 0.2).

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
prior <- 0.9
num_true <- prior*1000
pvals_true <- replicate(num_true, gen_ind_test(probA, probB, samp_size))


probA <- 0.2
probB <- 0.1
num_false <- ceiling((1-prior)*1000) # if I don't use celing it has a weird rounding effect
pvals_false <- replicate(num_false, gen_ind_test(probA, probB, samp_size))

```

a)  Report the fraction of rejected null hypotheses using alpha = 0.01 *without any p-value adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

b)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Bonferroni adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

c)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Holm adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

d)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Benjamini-Hochberg adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

```{r}
alpha <- 0.01

print("Fraction of rejections of true null without adjustment at alpha = 0.01:")
print(sum(pvals_true < alpha)/num_true)
FP <- sum(pvals_true < alpha)

print("Fraction of rejections of false null without adjustment at alpha = 0.01:")
print(sum(pvals_false < alpha)/num_false)
TP <- sum(pvals_false < alpha)

print("FDR without adjustment at alpha = 0.01:")
print(FP/(TP+FP))


pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'bonferroni')

print("Fraction of rejections of true null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Bonferroni adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'holm')

print("Fraction of rejections of true null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Holm adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'BH')

print("Fraction of rejections of true null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with BH adjustment at alpha = 0.01:")
print(FP/(TP+FP))
```

7.  Let us simulate a scenario where some fraction of the hypotheses are true (equal probabilities of disease) and some are false (different probabilities). Let us call the fraction of true hypotheses our prior. Set the prior probability to be 99%, and simulate 1000 hypotheses: 99% true and 1% false, with a small effect (probabilities of 0.1 and 0.2).

```{=html}
<!-- -->
```
a)  Report the fraction of rejected null hypotheses using alpha = 0.01 *without any p-value adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

b)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Bonferroni adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

c)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Holm adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

d)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Benjamini-Hochberg adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
prior <- 0.99
num_true <- prior*1000
pvals_true <- replicate(num_true, gen_ind_test(probA, probB, samp_size))


probA <- 0.2
probB <- 0.1
num_false <- ceiling((1-prior)*1000) # if I don't use celing it has a weird rounding effect
pvals_false <- replicate(num_false, gen_ind_test(probA, probB, samp_size))


alpha <- 0.01
print("Fraction of rejections of true null without adjustment at alpha = 0.01:")
print(sum(pvals_true < alpha)/num_true)
FP <- sum(pvals_true < alpha)

print("Fraction of rejections of false null without adjustment at alpha = 0.01:")
print(sum(pvals_false < alpha)/num_false)
TP <- sum(pvals_false < alpha)

print("FDR without adjustment at alpha = 0.01:")
print(FP/(TP+FP))


pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'bonferroni')

print("Fraction of rejections of true null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Bonferroni adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'holm')

print("Fraction of rejections of true null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Holm adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'BH')

print("Fraction of rejections of true null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with BH adjustment at alpha = 0.01:")
print(FP/(TP+FP))
```

8.  Let us simulate a scenario where some fraction of the hypotheses are true (equal probabilities of disease) and some are false (different probabilities). Let us call the fraction of true hypotheses our prior. Set the prior probability to be 90%, and simulate 1000 hypotheses: 90% true and 10% false, with a large effect (probabilities of 0.1 and 0.4).

<!-- -->

a)  Report the fraction of rejected null hypotheses using alpha = 0.01 *without any p-value adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

b)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Bonferroni adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

c)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Holm adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

d)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Benjamini-Hochberg adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
prior <- 0.9
num_true <- prior*1000
pvals_true <- replicate(num_true, gen_ind_test(probA, probB, samp_size))


probA <- 0.1
probB <- 0.4
num_false <- ceiling((1-prior)*1000) # if I don't use celing it has a weird rounding effect
pvals_false <- replicate(num_false, gen_ind_test(probA, probB, samp_size))


alpha <- 0.01
print("Fraction of rejections of true null without adjustment at alpha = 0.01:")
print(sum(pvals_true < alpha)/num_true)
FP <- sum(pvals_true < alpha)

print("Fraction of rejections of false null without adjustment at alpha = 0.01:")
print(sum(pvals_false < alpha)/num_false)
TP <- sum(pvals_false < alpha)

print("FDR without adjustment at alpha = 0.01:")
print(FP/(TP+FP))


pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'bonferroni')

print("Fraction of rejections of true null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Bonferroni adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'holm')

print("Fraction of rejections of true null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Holm adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'BH')

print("Fraction of rejections of true null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with BH adjustment at alpha = 0.01:")
print(FP/(TP+FP))
```

9.  Let us simulate a scenario where some fraction of the hypotheses are true (equal probabilities of disease) and some are false (different probabilities). Let us call the fraction of true hypotheses our prior. Set the prior probability to be 99%, and simulate 1000 hypotheses: 99% true and 1% false, with a large effect (probabilities of 0.1 and 0.4).

<!-- -->

a)  Report the fraction of rejected null hypotheses using alpha = 0.01 *without any p-value adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

b)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Bonferroni adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

c)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Holm adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

d)  Report the fraction of rejected null hypotheses using alpha = 0.01 *using the Benjamini-Hochberg adjustment* for each set of hypotheses, and report the total False Discovery Rate (the fraction of false positives out of all positive results.

```{r}
probA <- 0.2
probB <- 0.2
samp_size <- 200 # sample size
prior <- 0.99
num_true <- prior*1000
pvals_true <- replicate(num_true, gen_ind_test(probA, probB, samp_size))


probA <- 0.1
probB <- 0.4
num_false <- ceiling((1-prior)*1000) # if I don't use celing it has a weird rounding effect
pvals_false <- replicate(num_false, gen_ind_test(probA, probB, samp_size))

alpha <- 0.01
print("Fraction of rejections of true null without adjustment at alpha = 0.01:")
print(sum(pvals_true < alpha)/num_true)
FP <- sum(pvals_true < alpha)

print("Fraction of rejections of false null without adjustment at alpha = 0.01:")
print(sum(pvals_false < alpha)/num_false)
TP <- sum(pvals_false < alpha)

print("FDR without adjustment at alpha = 0.01:")
print(FP/(TP+FP))


pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'bonferroni')

print("Fraction of rejections of true null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Bonferroni adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Bonferroni adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'holm')

print("Fraction of rejections of true null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with Holm adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with Holm adjustment at alpha = 0.01:")
print(FP/(TP+FP))

pvals_adj <- p.adjust(c(pvals_true, pvals_false), method = 'BH')

print("Fraction of rejections of true null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[1:1000*prior] < alpha)/num_true)
FP <- sum(pvals_adj[1:1000*prior] < alpha)

print("Fraction of rejections of false null with BH adjustment at alpha = 0.01:")
print(sum(pvals_adj[(1000*prior):1000] < alpha)/num_false)
TP <- sum(pvals_adj[(1000*prior):1000] < alpha)

print("FDR with BH adjustment at alpha = 0.01:")
print(FP/(TP+FP))
```

10. Discuss the results you reported in the four questions above. Describe the influence of the following parameters: the effect size, the prior probability of the hypothesis being true, and the choice of p-value adjustment. Comment on your conclusions for what you would choose to do for a data set with a small effect size and a large prior probability for the null.

The results vary substantially for the two different effect sizes (small: 0.1 vs 0.2 and large: 0.1 vs 0.4), as well as the prior (fraction of true null hypotheses).

In the first case, small effect size and prior probability of 0.9, the hypothesis test detects about 1/2 (or more) of the false nulls using unadjusted p-values, and the FDR is low (a few percent), because there are not many false positives. All three adjustment methods are fairly ruthless about removing the positives, so we end up with very few of the low effect sizes detected after correction, though BH method is a bit more forgiving. All three give either a 0 FDR or a NA if all positives are removed.

In the second case, small effect size and prior probability of 0.99, the hypothesis test detects the same fraction of false nulls using unadjusted p-values, but the FDR is much higher (40-50%), because the true positives are more rare. All three adjustment methods are fairly ruthless about removing the positives, so we end up with very few of the low effect sizes detected after correction, though BH method is a bit more forgiving and can leave 10-20% true positive rate. Usually we see a zero FDR for Bonferroni or Holm, and a small but non-zero FDR with BH.

In the third case, large effect size and prior probability of 0.9, the hypothesis test with the unadjusted p-values detects virtually all of the false nulls, while keeping the same false positive rate of about 1%, so the FDR is very low. All three adjustment methods remove all of the (very few) false positive while keeping almost all of the true positives, so the FDR is zero for all three.

In the fourth case, large effect size and prior probability of 0.99, the hypothesis test with the unadjusted p-values detects virtually all of the false nulls, while keeping the same false positive rate of about 1%, but since there are very few true positives, the FDR can be as high as 40-50%. Once again, all three adjustment methods remove all of the (very few) false positive while keeping almost all of the true positives, so the FDR is zero for all three.

For the problem of detection of a low effect size with a low prevalence (high prior of the null hypothesis), I would choose the BH correction method, as it keeps a lot more of the true positives while weeding out virtually all of the false positives.
